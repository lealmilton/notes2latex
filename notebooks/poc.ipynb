{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import os\n",
    "import base64\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from src.image_processing.image_utils import convert_pdf_to_png, encode_image_to_base64\n",
    "from src.image_processing.ocr import send_image_to_gpt4v\n",
    "from src.text_processing.text_utils import get_previous_description, concatenate_text_files\n",
    "from src.utils.prompts import create_prompt_with_previous_description\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn PDF into separate pngs (one per pdf page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = '../data/notes/aula13.pdf'  # Replace with your PDF file path\n",
    "poppler_path = r'../venv/poppler-23.11.0/Library/bin'  # Specify your Poppler bin path\n",
    "\n",
    "convert_pdf_to_png(pdf_path, poppler_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract text from image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_png_images(folder_path, output_folder):\n",
    "    \"\"\"Processes all PNG images in the specified folder and saves the extracted text.\"\"\"\n",
    "    files = os.listdir(folder_path)\n",
    "    png_files = [file for file in files if file.endswith('.png')]\n",
    "    png_files_sorted = sorted(png_files, key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for file_index, file in enumerate(png_files_sorted, start=1):\n",
    "        image_path = os.path.join(folder_path, file)\n",
    "        print(image_path)\n",
    "        #break\n",
    "        previous_description = get_previous_description(ocr_text_folder, file_index)\n",
    "        prompt = create_prompt_with_previous_description(previous_description)\n",
    "        response = send_image_to_gpt4v(image_path, prompt)\n",
    "        time.sleep(15)\n",
    "        \n",
    "        extracted_text = response.get('choices', [{}])[0].get('message', {}).get('content', 'No text found')\n",
    "        output_file_path = os.path.join(output_folder, file.replace('.png', '.txt'))\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(extracted_text)\n",
    "\n",
    "        print(f\"Processed and saved OCR text for: {file}\")\n",
    "\n",
    "png_folder_path = \"../data/processed/aula13/pngs/\"\n",
    "ocr_text_folder = \"../data/processed/aula13/ocr_text/\"\n",
    "\n",
    "process_png_images(png_folder_path, ocr_text_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All text files have been concatenated into '../data/processed/aula13/combined_text.txt'.\n"
     ]
    }
   ],
   "source": [
    "ocr_text_folder = \"../data/processed/aula13/ocr_text\"\n",
    "output_file_path = \"../data/processed/aula13/combined_text.txt\"\n",
    "concatenate_text_files(ocr_text_folder, output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tiktoken import Tokenizer\n",
    "import os\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def measure_token_count(text):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def gpt4_completion(context, file_content, prompt):\n",
    "    client = OpenAI()  # Initialize the OpenAI client\n",
    "    full_prompt = context + file_content + \"\\n\\n\" + prompt\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a math editor.\"},\n",
    "            {\"role\": \"user\", \"content\": full_prompt}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def save_to_file(file_path, content):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "def split_text(file_content, split_size):\n",
    "    words = file_content.split()\n",
    "    return [' '.join(words[i:i + split_size]) for i in range(0, len(words), split_size)]\n",
    "\n",
    "# Configuration\n",
    "token_threshold = 3500  # Adjust as needed\n",
    "\n",
    "# File paths\n",
    "input_file_path = \"../data/processed/aula13/combined_text.txt\"\n",
    "output_file_path = \"../data/processed/aula13/formatted_notes.md\"\n",
    "\n",
    "# Read and measure the text\n",
    "file_content = read_file(input_file_path)\n",
    "total_tokens = measure_token_count(file_content)\n",
    "\n",
    "\n",
    "prompt = \"Dadas as anotações acima, crie uma versão limpa, bem elaborada e completamente fiel às anotações originais dessas notas de aula, mas faça isso em Inglês. \\\n",
    "    Seu output deve estar em linguagem Markdown, incluindo todo o código LaTeX quando necessário \\\n",
    "        e títulos e subtítulos apropriados. Cheque seu trabalho ao final e corrija qualquer erro de Latex. Lembre-se o output final é em Inglês. Verifique que não existe nenhum erro de Latex. Nao inclua as observações. Apenas um texto limpo em ingles em Markdown.\"\n",
    "\n",
    "# Determine processing strategy\n",
    "if total_tokens <= token_threshold:\n",
    "    # Process in one go\n",
    "    response = gpt4_completion(\"\", file_content, prompt)\n",
    "else:\n",
    "    # Split and process in chunks\n",
    "    split_size = token_threshold // 2\n",
    "    chunks = split_text(file_content, split_size)\n",
    "    context = \"\"\n",
    "    for chunk in chunks:\n",
    "        response_chunk = gpt4_completion(context, chunk, prompt)\n",
    "        context += response_chunk  # Update context with the output for the next chunk\n",
    "\n",
    "    response = context  # Final combined response\n",
    "\n",
    "# Save the response\n",
    "save_to_file(output_file_path, response)\n",
    "print(f\"Formatted notes saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def gpt4_completion(file_content, prompt):\n",
    "    client = OpenAI()  # Initialize the OpenAI client\n",
    "    full_prompt = file_content + \"\\n\\n\" + prompt\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-1106-preview\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a math editor.\"},\n",
    "            {\"role\": \"user\", \"content\": full_prompt}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "def save_to_file(file_path, content):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "def chunk_text(file_content, chunk_size, overlap_size):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(file_content):\n",
    "        end = min(start + chunk_size, len(file_content))\n",
    "        if end == len(file_content):\n",
    "            chunks.append(file_content[start:end])\n",
    "            break\n",
    "        overlap_end = min(end + overlap_size, len(file_content))\n",
    "        chunks.append(file_content[start:overlap_end])\n",
    "        start = end\n",
    "    return chunks\n",
    "\n",
    "def process_and_merge_chunks(chunks, prompt):\n",
    "    merged_content = \"\"\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        response_chunk = gpt4_completion(chunk, prompt)\n",
    "        if i > 0:\n",
    "            # Remove the overlapping part from the beginning of the current chunk's response\n",
    "            response_chunk = response_chunk[len(chunks[i-1]) - overlap_size:]\n",
    "        merged_content += response_chunk\n",
    "    return merged_content\n",
    "\n",
    "# Configuration\n",
    "chunk_size = 3000  # Adjust as needed\n",
    "overlap_size = 500  # Adjust as needed\n",
    "\n",
    "# File paths\n",
    "input_file_path = \"../data/processed/aula13/combined_text.txt\"\n",
    "output_file_path = \"../data/processed/aula13/formatted_notes.md\"\n",
    "\n",
    "# Read, chunk, and process the text\n",
    "file_content = read_file(input_file_path)\n",
    "chunks = chunk_text(file_content, chunk_size, overlap_size)\n",
    "processed_content = process_and_merge_chunks(chunks, prompt)\n",
    "\n",
    "# Save the final merged content\n",
    "save_to_file(output_file_path, processed_content)\n",
    "print(f\"Formatted notes saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted notes saved to ../data/processed/aula13/formatted_notes.md\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def gpt4_completion(file_content, prompt):\n",
    "    client = OpenAI()  # Initialize the OpenAI client\n",
    "\n",
    "    full_prompt = file_content + \"\\n\\n\" + prompt  # Combine file content and prompt\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-1106-preview\",  # Specify the GPT-4 model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a math editor.\"},\n",
    "            {\"role\": \"user\", \"content\": full_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extracting the response from the completion\n",
    "    response = completion.choices[0].message.content\n",
    "\n",
    "    return response\n",
    "\n",
    "def save_to_file(file_path, content):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "# Example usage\n",
    "input_file_path = \"../data/processed/aula13/combined_text.txt\"\n",
    "output_file_path = \"../data/processed/aula13/formatted_notes.md\"\n",
    "\n",
    "file_content = read_file(input_file_path)\n",
    "\n",
    "prompt = \"Dadas as anotações acima, crie uma versão limpa, bem elaborada e completamente fiel às anotações originais dessas notas de aula, mas faça isso em Inglês. \\\n",
    "    Seu output deve estar em linguagem Markdown, incluindo todo o código LaTeX quando necessário \\\n",
    "        e títulos e subtítulos apropriados. Cheque seu trabalho ao final e corrija qualquer erro de Latex. Lembre-se o output final é em Inglês. Verifique que não existe nenhum erro de Latex. Nao inclua as observações. Apenas um texto limpo em ingles em Markdown.\"\n",
    "\n",
    "response = gpt4_completion(file_content, prompt)\n",
    "\n",
    "# Save the response to a file\n",
    "save_to_file(output_file_path, response)\n",
    "\n",
    "print(f\"Formatted notes saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check LaTeX\n",
    "\n",
    "Use GPT-4 to check LaTeX syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn MarkDown into PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Pandoc died with exitcode \"43\" during conversion: pdflatex: major issue: So far, you have not checked for MiKTeX updates.\n",
      "pdflatex: major issue: So far, you have not checked for MiKTeX updates.\n",
      "Error producing PDF.\n",
      "! Package amsmath Error: \\dot allowed only in math mode.\n",
      "\n",
      "See the amsmath package documentation for explanation.\n",
      "Type  H <return>  for immediate help.\n",
      " ...                                              \n",
      "                                                  \n",
      "l.62 A System {[} \\dot{x}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pypandoc\n",
    "\n",
    "# This will download Pandoc to a location accessible by pypandoc\n",
    "pypandoc.download_pandoc()\n",
    "\n",
    "def convert_md_to_pdf(input_file_path, output_file_path):\n",
    "    try:\n",
    "        # Convert Markdown to PDF\n",
    "        pypandoc.convert_file(input_file_path, 'pdf', outputfile=output_file_path)\n",
    "        print(f\"PDF successfully created at {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage\n",
    "input_md_file = \"../data/processed/aula13/formatted_notes.md\"\n",
    "output_pdf_file = \"../data/processed/aula13/formatted_notes.pdf\"\n",
    "\n",
    "convert_md_to_pdf(input_md_file, output_pdf_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
